{"cells":[{"cell_type":"markdown","id":"juvenile-enforcement","metadata":{"id":"juvenile-enforcement"},"source":["# Week 1: Explore the BBC News archive\n","\n","Welcome! In this assignment you will be working with a variation of the [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview), which contains 2225 examples of news articles with their respective categories (labels).\n","\n","Let's get started!"]},{"cell_type":"code","execution_count":3,"id":"combined-brooklyn","metadata":{"id":"combined-brooklyn","tags":["graded"],"executionInfo":{"status":"ok","timestamp":1661650105179,"user_tz":-540,"elapsed":3475,"user":{"displayName":"Keisuke Morisada","userId":"05358334128248013544"}}},"outputs":[],"source":["import csv\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","id":"dependent-power","metadata":{"id":"dependent-power"},"source":["Begin by looking at the structure of the csv that contains the data:"]},{"cell_type":"code","execution_count":4,"id":"finite-panic","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"finite-panic","executionInfo":{"status":"error","timestamp":1661650107571,"user_tz":-540,"elapsed":759,"user":{"displayName":"Keisuke Morisada","userId":"05358334128248013544"}},"outputId":"4dee43f2-d392-4e2a-f85e-7d61f9d2d09c"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1cff8ceba275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/bbc-text.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"First line (header) looks like this:\\n\\n{csvfile.readline()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Each data point looks like this:\\n\\n{csvfile.readline()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/bbc-text.csv'"]}],"source":["with open(\"./data/bbc-text.csv\", 'r') as csvfile:\n","    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n","    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")     "]},{"cell_type":"markdown","id":"aggregate-calvin","metadata":{"id":"aggregate-calvin"},"source":["As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article."]},{"cell_type":"markdown","id":"rocky-credit","metadata":{"id":"rocky-credit"},"source":["## Removing Stopwords\n","\n","One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n","\n","Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided."]},{"cell_type":"code","execution_count":1,"id":"permanent-privilege","metadata":{"tags":["graded"],"id":"permanent-privilege","executionInfo":{"status":"ok","timestamp":1661649997494,"user_tz":-540,"elapsed":7,"user":{"displayName":"Keisuke Morisada","userId":"05358334128248013544"}}},"outputs":[],"source":["# GRADED FUNCTION: remove_stopwords\n","def remove_stopwords(sentence):\n","    \"\"\"\n","    Removes a list of stopwords\n","    \n","    Args:\n","        sentence (string): sentence to remove the stopwords from\n","    \n","    Returns:\n","        sentence (string): lowercase sentence without the stopwords\n","    \"\"\"\n","    # List of stopwords\n","    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","    \n","    # Sentence converted to lowercase-only\n","    sentence = sentence.lower()\n","    \n","    ### START CODE HERE\n","    sentence_char_list = sentence.split(\" \")\n","    s_list = [i for i in sentence_char_list if i not in stopwords]\n","    sentence = ' '.join(s_list)\n","    ### END CODE HERE\n","    return sentence"]},{"cell_type":"code","execution_count":2,"id":"northern-third","metadata":{"tags":["graded"],"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"northern-third","executionInfo":{"status":"ok","timestamp":1661650000738,"user_tz":-540,"elapsed":9,"user":{"displayName":"Keisuke Morisada","userId":"05358334128248013544"}},"outputId":"e58b827b-8967-4a17-bd64-d713ae197f14"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'go store get snack'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["# Test your function\n","remove_stopwords(\"I am about to go to the store and get any snack\")"]},{"cell_type":"markdown","id":"pressed-excellence","metadata":{"id":"pressed-excellence"},"source":["***Expected Output:***\n","```\n","'go store get snack'\n","\n","```"]},{"cell_type":"markdown","id":"animal-photography","metadata":{"id":"animal-photography"},"source":["## Reading the raw data\n","\n","Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n","\n","A couple of things to note:\n","- You should omit the first line as it contains the headers and not data points.\n","- There is no need to save the data points as numpy arrays, regular lists is fine.\n","- To read from csv files use [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader) by passing the appropriate arguments.\n","- `csv.reader` returns an iterable that returns each row in every iteration. So the label can be accessed via row[0] and the text via row[1].\n","- Use the `remove_stopwords` function in each sentence."]},{"cell_type":"code","execution_count":null,"id":"monthly-beginning","metadata":{"tags":["graded"],"id":"monthly-beginning"},"outputs":[],"source":["# GRADED FUNCTION: parse_data_from_file\n","def parse_data_from_file(filename):\n","    \"\"\"\n","    Extracts sentences and labels from a CSV file\n","    \n","    Args:\n","        filename (string): path to the CSV file\n","    \n","    Returns:\n","        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n","    \"\"\"\n","    sentences = []\n","    labels = []\n","    with open(filename, 'r') as csvfile:\n","        ### START CODE HERE\n","        reader = csv.reader(csvfile, delimiter=',')\n","        l = [row for row in reader]\n","        for row in l[1:]:\n","            sentences.append(remove_stopwords(row[1]))\n","            labels.append(row[0])\n","        ### END CODE HERE\n","    return sentences, labels"]},{"cell_type":"code","execution_count":null,"id":"listed-saturn","metadata":{"tags":["graded"],"id":"listed-saturn"},"outputs":[],"source":["# Test your function\n","\n","# With original dataset\n","sentences, labels = parse_data_from_file(\"./data/bbc-text.csv\")\n","\n","print(\"ORIGINAL DATASET:\\n\")\n","print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n","print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n","print(f\"There are {len(labels)} labels in the dataset.\\n\")\n","print(f\"The first 5 labels are {labels[:5]}\\n\\n\")\n","\n","# With a miniature version of the dataset that contains only first 5 rows\n","mini_sentences, mini_labels = parse_data_from_file(\"./data/bbc-text-minimal.csv\")\n","\n","print(\"MINIATURE DATASET:\\n\")\n","print(f\"There are {len(mini_sentences)} sentences in the miniature dataset.\\n\")\n","print(f\"First sentence has {len(mini_sentences[0].split())} words (after removing stopwords).\\n\")\n","print(f\"There are {len(mini_labels)} labels in the miniature dataset.\\n\")\n","print(f\"The first 5 labels are {mini_labels[:5]}\")"]},{"cell_type":"markdown","id":"favorite-shanghai","metadata":{"id":"favorite-shanghai"},"source":["***Expected Output:***\n","```\n","ORIGINAL DATASET:\n","\n","There are 2225 sentences in the dataset.\n","\n","First sentence has 436 words (after removing stopwords).\n","\n","There are 2225 labels in the dataset.\n","\n","The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n","\n","\n","MINIATURE DATASET:\n","\n","There are 5 sentences in the miniature dataset.\n","\n","First sentence has 436 words (after removing stopwords).\n","\n","There are 5 labels in the miniature dataset.\n","\n","The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n","\n","```"]},{"cell_type":"markdown","id":"diverse-basket","metadata":{"id":"diverse-basket"},"source":["## Using the Tokenizer\n","\n","Now it is time to tokenize the sentences of the dataset. \n","\n","Complete the `fit_tokenizer` below. \n","\n","This function should receive the list of sentences as input and return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to those sentences. You should also define the \"Out of Vocabulary\" token as `<OOV>`."]},{"cell_type":"code","execution_count":null,"id":"cultural-virginia","metadata":{"tags":["graded"],"id":"cultural-virginia"},"outputs":[],"source":["# GRADED FUNCTION: fit_tokenizer\n","def fit_tokenizer(sentences):\n","    \"\"\"\n","    Instantiates the Tokenizer class\n","    \n","    Args:\n","        sentences (list): lower-cased sentences without stopwords\n","    \n","    Returns:\n","        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n","    \"\"\"\n","    ### START CODE HERE\n","    # Instantiate the Tokenizer class by passing in the oov_token argument\n","    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","    # Fit on the sentences\n","    tokenizer.fit_on_texts(sentences)\n","    \n","    ### END CODE HERE\n","    return tokenizer"]},{"cell_type":"code","execution_count":null,"id":"tracked-hostel","metadata":{"tags":["graded"],"id":"tracked-hostel"},"outputs":[],"source":["tokenizer = fit_tokenizer(sentences)\n","word_index = tokenizer.word_index\n","\n","print(f\"Vocabulary contains {len(word_index)} words\\n\")\n","print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"]},{"cell_type":"markdown","id":"moderate-pollution","metadata":{"id":"moderate-pollution"},"source":["***Expected Output:***\n","```\n","Vocabulary contains 29714 words\n","\n","<OOV> token included in vocabulary\n","\n","```"]},{"cell_type":"code","execution_count":null,"id":"golden-flash","metadata":{"tags":["graded"],"id":"golden-flash"},"outputs":[],"source":["# GRADED FUNCTION: get_padded_sequences\n","def get_padded_sequences(tokenizer, sentences):\n","    \"\"\"\n","    Generates an array of token sequences and pads them to the same length\n","    \n","    Args:\n","        tokenizer (object): Tokenizer instance containing the word-index dictionary\n","        sentences (list of string): list of sentences to tokenize and pad\n","    \n","    Returns:\n","        padded_sequences (array of int): tokenized sentences padded to the same length\n","    \"\"\"\n","    \n","    ### START CODE HERE\n","    # Convert sentences to sequences\n","    sequences = tokenizer.texts_to_sequences(sentences)\n","    \n","    # Pad the sequences using the post padding strategy\n","    padded_sequences = pad_sequences(sequences, padding='post')\n","    ### END CODE HERE\n","    \n","    return padded_sequences"]},{"cell_type":"code","execution_count":null,"id":"spanish-entrepreneur","metadata":{"tags":["graded"],"id":"spanish-entrepreneur"},"outputs":[],"source":["padded_sequences = get_padded_sequences(tokenizer, sentences)\n","print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n","print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n","print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")"]},{"cell_type":"markdown","id":"wired-brief","metadata":{"id":"wired-brief"},"source":["***Expected Output:***\n","```\n","First padded sequence looks like this: \n","\n","[  96  176 1157 ...    0    0    0]\n","\n","Numpy array of all sequences has shape: (2225, 2438)\n","\n","This means there are 2225 sequences in total and each one has a size of 2438\n","\n","```"]},{"cell_type":"code","execution_count":null,"id":"unknown-optimization","metadata":{"tags":["graded"],"id":"unknown-optimization"},"outputs":[],"source":["# GRADED FUNCTION: tokenize_labels\n","def tokenize_labels(labels):\n","    \"\"\"\n","    Tokenizes the labels\n","    \n","    Args:\n","        labels (list of string): labels to tokenize\n","    \n","    Returns:\n","        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n","    \"\"\"\n","    ### START CODE HERE\n","    \n","    # Instantiate the Tokenizer class\n","    # No need to pass additional arguments since you will be tokenizing the labels\n","    label_tokenizer = Tokenizer()\n","    \n","    # Fit the tokenizer to the labels\n","    label_tokenizer.fit_on_texts(labels)\n","    \n","    # Save the word index\n","    label_word_index = label_tokenizer.word_index\n","    \n","    # Save the sequences\n","    label_sequences = label_tokenizer.texts_to_sequences(labels)\n","\n","    ### END CODE HERE\n","    \n","    return label_sequences, label_word_index"]},{"cell_type":"code","execution_count":null,"id":"streaming-conviction","metadata":{"tags":["graded"],"id":"streaming-conviction"},"outputs":[],"source":["label_sequences, label_word_index = tokenize_labels(labels)\n","print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n","print(f\"First ten sequences {label_sequences[:10]}\\n\")"]},{"cell_type":"markdown","id":"endangered-poultry","metadata":{"id":"endangered-poultry"},"source":["***Expected Output:***\n","```\n","Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n","\n","First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n","\n","```"]},{"cell_type":"markdown","id":"cross-chess","metadata":{"id":"cross-chess"},"source":["**Congratulations on finishing this week's assignment!**\n","\n","You have successfully implemented functions to process various text data processing ranging from pre-processing, reading from raw files and tokenizing text.\n","\n","**Keep it up!**"]}],"metadata":{"dlai_version":"1.2.0","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"C3W1_Assignment.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}